# TogoMCP Usage Guide

## ‚ö° QUICK START (Read This First)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ What's your question type?                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ "Which has MOST/LEAST/MORE?"                    ‚îÇ
‚îÇ ‚Üí Use COMPARATIVE WORKFLOW below                ‚îÇ
‚îÇ                                                  ‚îÇ
‚îÇ "How many...", "Find all...", "List..."         ‚îÇ
‚îÇ ‚Üí Use STANDARD WORKFLOW below                   ‚îÇ
‚îÇ                                                  ‚îÇ
‚îÇ "Find protein TP53", "Get details for..."       ‚îÇ
‚îÇ ‚Üí Use search tools (skip to Tools section)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîç STEP 0: DATABASE DISCOVERY (ALWAYS START HERE!)

**Call `list_databases()` to discover which databases contain your data:**

```python
list_databases()  # Returns: 23 databases with descriptions

# Match keywords in descriptions to your query:
# "MANE" ‚Üí Ensembl (not just NCBI Gene!)
# "drug targets" + "approved" ‚Üí ChEMBL
# "clinical variants" ‚Üí ClinVar
# "pathways" ‚Üí Reactome
```

### Quick Workflow

```python
# 1. Discover databases by keywords
list_databases()  # Read descriptions

# 2. Check if they share endpoints (for cross-DB queries)
get_sparql_endpoints()

# 3. Get schemas for identified databases
get_MIE_file('ensembl')  # Found "MANE" in description
get_MIE_file('chembl')   # Found "drug targets" in description

# 4. Query using discovered structured properties
```

### Common Mistake

‚ùå **Assuming:** "MANE is NCBI, so use ncbigene" ‚Üí Empty results  
‚úÖ **Discovering:** `list_databases()` shows "MANE" in Ensembl ‚Üí Success

**When to use:** Starting new queries, uncertain about database, query has multiple concepts, or getting empty results  
**When to skip:** Already know exact database from prior work in same session

**Rule of thumb:** 5 seconds of discovery prevents hours of debugging

---

## üîå ENDPOINT ARCHITECTURE (CRITICAL!)

**Before any multi-database query, check if databases share an endpoint:**

```python
get_sparql_endpoints()
# Returns: which databases are on which endpoints
```

### Can Query Together (Same Endpoint)
‚úÖ **ncbi endpoint:** ClinVar + PubMed + PubTator + NCBI Gene + MedGen  
‚úÖ **primary endpoint:** MeSH + GO + Taxonomy + MONDO + NANDO  
‚úÖ **ebi endpoint:** ChEMBL + ChEBI + Reactome + Ensembl  
‚úÖ **sib endpoint:** UniProt + Rhea  

### CANNOT Query Together (Different Endpoints)
‚ùå NCBI Gene (ncbi) + Taxonomy (primary)  
‚ùå UniProt (sib) + ChEMBL (ebi)  
‚ùå PubChem (pubchem) + any other database  

### When Databases Are on Different Endpoints

**You MUST use hybrid approach:**

```python
# Example: Count genes by phylum (ncbigene + taxonomy on different endpoints)

# Step 1: Get ALL gene IDs (use API, not SPARQL)
all_genes = []
for offset in range(0, total_count, 100):
    batch = ncbi_esearch("gene", query, max_results=100, start_index=offset)
    all_genes.extend(batch)  # Don't stop at 100! Get ALL results

# Step 2: Get organism taxids (batch process)
phylum_counts = {}
for gene_batch in batches(all_genes, 50):
    summaries = ncbi_esummary("gene", gene_batch)
    taxids = [s['organism']['taxid'] for s in summaries]
    
    # Step 3: Map taxids to phyla (now can use SPARQL on taxonomy)
    for taxid in taxids:
        phylum = run_sparql(dbname="taxonomy", query=f"""
            SELECT ?phylum WHERE {{
                taxon:{taxid} rdfs:subClassOf+ ?phylum .
                ?phylum tax:rank tax:Phylum .
            }}
        """)
        phylum_counts[phylum] = phylum_counts.get(phylum, 0) + 1

# Step 4: Compare
sorted(phylum_counts.items(), key=lambda x: x[1], reverse=True)
```

**‚ö†Ô∏è CRITICAL: Process ALL results, not just samples!**

### Batch Processing Implementation

```python
# COMPLETE PATTERN: Process 1,000+ items efficiently
all_gene_ids = []  # From ncbi_esearch (e.g., 1,367 IDs)
phylum_counts = {}

# Batch 1: ncbi_esummary (50-100 IDs per call)
for i in range(0, len(all_gene_ids), 50):
    batch_ids = all_gene_ids[i:i+50]
    summaries = ncbi_esummary("gene", batch_ids)
    taxids = [s['organism']['taxid'] for s in summaries]
    
    # Batch 2: SPARQL (10-15 taxids per query)
    for j in range(0, len(taxids), 10):
        taxid_batch = taxids[j:j+10]
        query = f"""
            VALUES ?taxid {{ {' '.join([f'taxon:{t}' for t in taxid_batch])} }}
            ?taxid rdfs:subClassOf+ ?phylum .
            ?phylum tax:rank tax:Phylum .
        """
        results = run_sparql(dbname="taxonomy", query=query)
        for result in results:
            phylum_counts[result['phylumLabel']] += 1

# Total calls: ~(1367/50 + 1367/10) = ~27 + 137 = ~164 calls
```

**Performance Notes:**
- **ncbi_esummary**: 50-100 IDs optimal, max ~200
- **SPARQL VALUES**: 10-15 items optimal, fails >20
- **Rate limits**: None on TogoMCP; 60s timeout per query
- **Expected scale**: 50-200 calls for 1,000 items is normal

**Error Recovery:**
```python
# Save progress for long-running queries
import json
for batch_num, batch in enumerate(batches(all_items, 50)):
    # Process batch...
    phylum_counts[phylum] += 1
    
    # Save after each batch (every ~50 items)
    if batch_num % 10 == 0:  # Every 500 items
        with open('/tmp/progress.json', 'w') as f:
            json.dump(phylum_counts, f)
```

---

## üö® COMPARATIVE WORKFLOW ("which has MOST/LEAST")

**Use this for:** "Which phylum has most genes?", "Which organism has more proteins?"

### Critical Rule: Don't Be Circular!

**‚ùå WRONG (Circular Reasoning):**
```python
# 1. Search finds 100 examples from category A
# 2. Count only category A: "A has the most!"
# ‚ùå Problem: Never checked categories B, C, D!
```

**‚úÖ RIGHT (Systematic):**
```python
# 1. Get MIE ‚Üí find structured properties
# 2. ENUMERATE ALL categories (A, B, C, D...)
# 3. Use BROAD search: "(term1 OR term2 OR description)"
# 4. Count in EACH category
# 5. Compare systematically
```

### 6-Step Checklist

‚òê **0. Check endpoints** ‚Üí `get_sparql_endpoints()` if multi-database  
‚òê **1. Get MIE file** ‚Üí find structured properties  
‚òê **2. Enumerate ALL categories** ‚Üí don't assume, list them!  
‚òê **3. Broad search query** ‚Üí use OR: `"(nifH OR 'nitrogenase iron protein')"`  
‚òê **4. Count EACH category** ‚Üí process ALL results, not samples!  
‚òê **5. Compare** ‚Üí ORDER BY DESC(?count) to find winner  

### When to Stop vs Continue Processing

**‚ùå NEVER stop early (100% required):**
- **Comparative**: "Which has MOST/LEAST?" ‚Üí Must count ALL
- **Exact counts**: "How many total..." ‚Üí Must count ALL
- **Rankings**: "Top 10...", "Rank by..." ‚Üí Must count ALL
- **Factoid answers**: Any definitive answer ‚Üí Must be comprehensive

**‚úÖ OK to stop early (sampling acceptable):**
- **Exploratory**: "Are there ANY X?" ‚Üí Stop after finding examples
- **Approximate**: "Roughly how many..." ‚Üí Representative sample OK

**Rule**: Questions with "most", "least", "all", "none", "exact", or requiring ranking ‚Üí process 100%

**Example 1: Same Endpoint (Single SPARQL)**
```python
# Q: "Which organism has more kinases - human or mouse?"
# UniProt is all on 'sib' endpoint - can use single query

get_MIE_file('uniprot')  # up:organism, up:classifiedWith
get_sparql_endpoints()   # Confirm: uniprot on 'sib' endpoint

query = """
SELECT ?organism (COUNT(?p) as ?count) WHERE {
  ?p up:reviewed 1 ;
     up:classifiedWith <http://purl.uniprot.org/keywords/418> ;
     up:organism ?organism .
  FILTER(?organism IN (
    <http://purl.uniprot.org/taxonomy/9606>,
    <http://purl.uniprot.org/taxonomy/10090>
  ))
}
GROUP BY ?organism ORDER BY DESC(?count)
"""
```

**Example 2: Different Endpoints (Hybrid Approach)**
```python
# Q: "Which archaeal phylum has most nifH genes?"
# ncbigene (ncbi) + taxonomy (primary) = different endpoints!

get_sparql_endpoints()  # Confirm: different endpoints
get_MIE_file('taxonomy')  # Find: tax:rank, rdfs:subClassOf

# Step 1: Enumerate all phyla (taxonomy DB)
phyla = run_sparql(dbname="taxonomy", query="""
  SELECT DISTINCT ?phylum WHERE {
    ?phylum tax:rank tax:Phylum ;
            rdfs:subClassOf+ taxon:2157 .
  }
""")  # Found: 12 phyla

# Step 2: Get ALL genes (API - can't join in SPARQL)
all_genes = []
for offset in range(0, 1367, 100):  # Don't stop at 100!
    batch = ncbi_esearch("gene", 
                         "Archaea[Organism] AND (nifH[Gene Name] OR nitrogenase)",
                         max_results=100, start_index=offset)
    all_genes.extend(batch)

# Step 3: Count each phylum (batch process ALL)
phylum_counts = {}
for gene_batch in batches(all_genes, 50):
    summaries = ncbi_esummary("gene", gene_batch)
    for gene in summaries:
        taxid = gene['organism']['taxid']
        # Map to phylum via taxonomy DB
        result = run_sparql(dbname="taxonomy", query=f"""
            SELECT ?phylum WHERE {{
                taxon:{taxid} rdfs:subClassOf+ ?phylum .
                ?phylum tax:rank tax:Phylum .
            }}
        """)
        if result:
            phylum_counts[result[0]] = phylum_counts.get(result[0], 0) + 1

# Result: Methanobacteriota: 1,298, Thermoproteota: 18, ...
```

---

## üìã STANDARD WORKFLOW (comprehensive queries)

**Use this for:** "How many...", "Find all...", "Are there any..."

### 3-Step Process

**1. GET MIE FILE FIRST (MANDATORY)**
```python
get_MIE_file('dbname')
# Check: shape_expressions section
# Find: classification predicates, external IRIs, typed predicates
```

**2. SEARCH FOR EXAMPLES**
- Use search tools if listed in `kw_search_tools`
- Or exploratory SPARQL (LIMIT 10)
- **Use OR logic:** `"(term OR synonym OR description)"`

**3. INSPECT & QUERY**
```sparql
# Inspect examples:
SELECT * WHERE { VALUES ?entity {...} ?entity ?p ?o } LIMIT 100

# Then comprehensive query using discovered properties
SELECT (COUNT(?x) as ?count) WHERE {
  ?x structured_property <value> .  # From MIE schema
}
```

### Before SPARQL: Gate Check

‚ùì Multi-database? Checked endpoints first?  
‚ùì Got MIE file?  
‚ùì Found structured property in schema?  
‚ùì Inspected examples to confirm?  
‚ùì Using structured predicates (not bif:contains)?  

**NO to any? ‚Üí STOP, complete that step**

---

## üîë KEY RULES

### Rule 0: Database Discovery First
**Call `list_databases()` to identify relevant databases**

- Read database descriptions for keyword matches
- Identify 1-3 databases before calling MIE files
- **Never assume** a database contains data without checking
- Prevents 50-80% of "empty results" errors

### Rule 1: Check Endpoints for Multi-Database Queries
**Call `get_sparql_endpoints()` before combining databases**

- Same endpoint ‚Üí single SPARQL with multiple GRAPHs
- Different endpoints ‚Üí hybrid approach (API + SPARQL)
- **Never assume** databases can be joined

### Rule 2: MIE File First
**95% of failures = skipping `get_MIE_file()`**

Check these in schema:
- Classification predicates (atcClassification, classifiedWith)
- External IRIs (taxonomy, MeSH, GO terms)
- Typed predicates (assayType, status, phase)
- Hierarchies (subClassOf, pathwayComponent)

### Rule 3: Use OR Logic for Broad Searches
**Wrong:** `"nifH"` ‚Üí finds 286 genes (21%)  
**Right:** `"(nifH OR 'nitrogenase iron protein')"` ‚Üí finds 1,367 genes (100%)

### Rule 4: Structured > Text Search
```
Priority: Structured Properties > bif:contains
            (ALWAYS)              (RARE <5%)
```

**bif:contains ONLY if:** No structured alternative exists after checking MIE + inspecting examples

### Rule 5: No Circular Reasoning
**Never:**
- Search ‚Üí get examples ‚Üí query ONLY those examples
- That's circular - you only checked what you found!

**Always:**
- For comparisons: enumerate ALL categories first
- Use structured predicates to search entire database

---

## üõ†Ô∏è TOOLS REFERENCE

### Schema & Discovery
- `list_databases()` - **üîç CALL THIS FIRST to discover which databases to use**
- `get_MIE_file(dbname)` - **Get schema for identified databases**
- `get_sparql_endpoints()` - **Check before multi-database queries**

### Search (Exploratory)
- `ncbi_esearch(database, query)` - NCBI databases
- `search_uniprot_entity(query)` - Proteins
- `search_chembl_molecule(query)` - Drugs
- `search_chembl_target(query)` - Drug targets
- `search_pdb_entity(db, query)` - Structures
- `search_reactome_entity(query)` - Pathways
- `search_rhea_entity(query)` - Reactions

### SPARQL
- `run_sparql(dbname, query)` - Execute query

### ID Conversion
- `togoid_convertId(ids, route)` - Convert between databases

---

## üìñ QUICK EXAMPLES

### Example 1: Simple Count
```python
# Q: "How many human proteins?"
get_MIE_file('uniprot')  # Found: up:organism predicate
query = """
SELECT (COUNT(?p) as ?count) WHERE {
  ?p up:reviewed 1 ;
     up:organism <http://purl.uniprot.org/taxonomy/9606> .
}
"""
```

### Example 2: Comparative
```python
# Q: "Which organism has more kinases - human or mouse?"
get_MIE_file('uniprot')  # Found: up:classifiedWith, up:organism

# Don't search and count only results!
# Instead: query both systematically
query = """
SELECT ?organism (COUNT(?p) as ?count) WHERE {
  ?p up:reviewed 1 ;
     up:classifiedWith <http://purl.uniprot.org/keywords/418> ;
     up:organism ?organism .
  FILTER(?organism IN (
    <http://purl.uniprot.org/taxonomy/9606>,
    <http://purl.uniprot.org/taxonomy/10090>
  ))
}
GROUP BY ?organism
"""
```

### Example 3: With Classification
```python
# Q: "How many antibiotics in ChEMBL?"
get_MIE_file('chembl')  # Found: cco:atcClassification

# Don't search "antibiotic" in text!
# Use classification:
query = """
PREFIX cco: <http://rdf.ebi.ac.uk/terms/chembl#>
SELECT (COUNT(?m) as ?count) 
FROM <http://rdf.ebi.ac.uk/dataset/chembl>
WHERE {
  ?m cco:atcClassification ?atc .
  FILTER(STRSTARTS(?atc, "J01"))
}
"""
```

---

## üö´ TOP 5 MISTAKES

### 1. Skipping Database Discovery
**Impact:** Query wrong database, miss 50-80% of relevant data  
**Example:** Query "MANE Select" but only check NCBI Gene, miss Ensembl entirely  
**Fix:** Call `list_databases()` first, read descriptions for keywords

### 2. Assuming Cross-Database SPARQL Works
**Impact:** Query fails, confusion about why  
**Fix:** Call `get_sparql_endpoints()` first

### 3. Skipping MIE File
**Impact:** Wasted 1 hour ‚Üí could be 1 minute

### 4. Sampling Instead of Exhaustive Processing
**Impact:** Wrong answer - counted 46/1,367 organisms (3% sample) and claimed "confident"  
**Why wrong:** For "which has MOST", 91% sample confidence ‚â† 100% accurate count  
**Fix:** Process ALL results with pagination (~164 API calls for 1,367 items is normal)

### 5. Using bif:contains Without Checking Schema
**Impact:** 10-100x slower, incomplete results

---

## üÜò TROUBLESHOOTING

| Problem | Fix |
|---------|-----|
| Cross-DB query fails | Check `get_sparql_endpoints()` - use hybrid if different |
| Empty results | Check MIE schema for property names |
| Timeout | Add LIMIT, use structured predicates |
| Incomplete results | Did you use OR logic? Process ALL results? |
| "Which has most" is wrong | Did you enumerate ALL categories & process ALL results? |

---

---

## üö´ ERROR HANDLING

### Common Batch Processing Errors

```python
# Pattern: Robust batch processing
failed_items = []
for batch in batches(all_items, batch_size):
    try:
        results = process_batch(batch)
        for item, result in zip(batch, results):
            if result:  # Valid result
                counts[result] += 1
            else:  # Missing/null result
                failed_items.append(item)
    except Exception as e:
        print(f"Batch failed: {e}")
        failed_items.extend(batch)

if failed_items:
    print(f"Warning: {len(failed_items)} items failed processing")
```

**Common issues:**
- **Missing data**: New organisms lacking phylum classification ‚Üí skip and log
- **Timeout**: SPARQL query too large ‚Üí reduce batch size to 5-10
- **Empty results**: Taxid not in database ‚Üí normal, continue processing

---

## üéØ REMEMBER

**Call list_databases() to discover relevant databases - this is reconnaissance, not optional**

**Check endpoints FIRST for multi-database queries**

**The MIE file is your map - read it before querying**

**Process ALL results, not samples - pagination is your friend**

**Comparisons: Enumerate ALL ‚Üí Count EACH ‚Üí Compare**

**Broad searches: Use OR logic for synonyms**

**1 minute planning > 1 hour debugging**

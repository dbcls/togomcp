# TogoMCP QA Review Prompt

> **Usage:** Paste this prompt into a new Claude session. No file attachment needed â€” Claude reads everything automatically and updates this file when done.

---

You are a strict QA reviewer for a TogoMCP evaluation question set.

**Step 1 â€” Find the next unreviewed question.**
Read the progress tracker table at the bottom of this prompt. Find the first question whose Status is `â€”` (not yet reviewed). That is your target question number (e.g., `007`).

**Step 2 â€” Read the question file.**
Use the `Filesystem:read_text_file` tool to read:
```
/Users/arkinjo/work/GitHub/togo-mcp/evaluation2/questions/question_XXX.yaml
```
(replace `XXX` with the zero-padded number you identified in Step 1).

**Step 3 â€” Run all checks.**
Read the file in full, then check every error category below.

For each issue found, report:
- **Error code** (e.g., C02)
- **Evidence** â€” exact quote from the YAML field
- **Explanation** â€” why this violates the creation guide

At the end, give a **Verdict**: `PASS` | `MINOR ISSUES` | `MAJOR ERRORS` with a one-sentence summary.

---

## Error Categories

### ðŸ”´ CRITICAL

**[C01] Circular Logic**
Is the ideal_answer derived from the same query used to construct the question? Could the question only be answered by re-running the exploration that produced it â€” i.e., is it self-fulfilling?

**[C02] Coverage Gap â€” Query scope narrower than question scope**
For questions asking "which X", "how many X", "list all X", yes/no existence, or summary questions:
- If vocabulary exploration discovered N ontology terms (GO, MONDO, ChEBI, etc.), does the SPARQL `VALUES` clause use **all** N terms?
- If exploration found a set of entities, does the query process **all** of them â€” not a sampled subset?
- Does the question ask comprehensively while the SPARQL only covers a portion?

**[C03] Arithmetic Verification Missing or Failed**
If any SPARQL query uses `GROUP BY` or counts by category:
- Is there a verification query confirming `sum of category counts = total unique entities`?
- If `sum > total`, is the overlap explained in `ideal_answer`?
- If `sum < total`, was the coverage gap caught and fixed?
- If `GROUP BY` is present but no verification query exists, flag it.

**[C04] Vocabulary Sampling**
- Were ontology terms discovered (GO, MONDO, ChEBI, MeSH, EC)?
- Were `OLS4:getDescendants()` results obtained?
- Does the SPARQL `VALUES` include **all** discovered terms, including all descendants?

**[C05] Unverified Filter Heuristic**
Were entities filtered using taxonomy ID ranges, name-pattern matching, or manual "eyeballing" â€” without a count-verification query confirming `(before filter) = (filtered) + (remaining)`?

**[C06] Reverse Engineering**
Is the question scope (e.g., "SLE-associated genes") broader than the set of entities actually queried (e.g., only one of several SLE genes)?

**[C22] Literature-Recoverable Answer**
Could a PubMed search + abstract reading fully answer this question without querying any RDF database? If yes, check whether `rdf_necessity` score is â‰¥ 2. Flag if score is 0 or 1.

**[C23] Biological Insight = 0**
Is `biological_insight` scored 0 or 1? Does the question merely inventory database contents with no mechanistic, functional, or evolutionary insight?

---

### ðŸŸ  MAJOR

**[C07] Famous Entity**
Does the question body involve BRCA1, TP53, insulin, aspirin, glycolysis, E. coli K-12, or other canonically famous examples explicitly prohibited by the guide?

**[C08] Wrong Question Type**
Does the `type` label match the actual question structure?
- `yes_no` â†’ binary, no enumeration required
- `factoid` â†’ single value or count
- `list` â†’ enumerable set
- `summary` â†’ multi-dimensional aggregation, single-paragraph answer
- `choice` â†’ comparison among bounded options

**[C09] Type Distribution Violation**
Does this `type` have more than 12 uses across all 50 questions (hard cap)? Check whether `type` was already at â‰¥ 10 uses when a different under-represented type (< 8 uses) should have been prioritized.

**[C10] Structured Vocabulary Check Skipped**
Was `bif:contains` or free-text search used for a concept where GO / MONDO / ChEBI / EC / MeSH should have been checked first? Is there a note in the YAML documenting why no structured IRI was available?

**[C11] Descendants Not Fetched**
If an ontology term (GO, MONDO, etc.) was found, is there evidence that `OLS4:getDescendants()` was called and all descendants were included in the query?

**[C12] PubMed Test Invalid**
Did `pubmed_test` actually attempt to **answer** the specific question (retrieve the count, list, or fact), or did it only confirm the topic exists in literature? A valid test must try â€” and fail â€” to retrieve the specific answer.

**[C13] Single Database Only**
Does `togomcp_databases_used` contain only one database? If so, is there documented justification?

**[C14] Database Post-Selection**
Were databases chosen because results appeared there during keyword exploration, rather than for biological complementarity defined before exploration began?

**[C15] Format Error â€” `exact_answer` type**
- `yes_no` â†’ string `"yes"` or `"no"`
- `factoid` â†’ single string or number
- `list` â†’ YAML array (even for a single item)
- `choice` â†’ YAML array, items must exist in the `choices` field
- `summary` â†’ empty string `""`

**[C16] Format Error â€” SPARQL queries**
Each entry in `sparql_queries` must have: `query_number` (sequential), `database`, `description`, `query`, `result_count`.

**[C19] Inventory Question**
Does the question ask about database structure or metadata rather than biology? (e.g., "How many entries does UniProt have for kinases?" = inventory = prohibited.)

**[C21] Unbounded Scope**
For `list` or `summary` questions: is the scope verifiable (5â€“100 members), or is there a stated top-N justification? Flag if neither applies.

---

### ðŸŸ¡ MINOR

**[C17] Format Error â€” RDF triples**
Every triple must be followed by a comment in the form:
```
# Database: X | Query: N | Comment: ...
```
Flag any triple without this annotation.

**[C18] Vague Wording**
Does the `body` use "bind", "contain", "have", "associated with", or "interact with" without a qualifying adjective (e.g., "annotated with", "co-crystallized with", "documented")?

**[C20] Undocumented Overlap**
If `sum of GROUP BY counts > total unique entities`, is the shared membership explicitly explained in `ideal_answer`?

**[C24] Keyword-Workflow Violated**
Does the question type feel force-fitted to the keyword topic, suggesting the keyword was chosen before type and database selection? (Assess from internal consistency of `inspiration_keyword`, `type`, and `togomcp_databases_used`.)

**[C25] UniProt Cap**
Is UniProt listed in `togomcp_databases_used`? If this question pushes cumulative UniProt usage past 35 of 50 questions (70%), flag it.

---

## Output Format

```
## question_XXX QA Report

### Issues Found

**[Cxx] Category Name** (CRITICAL / MAJOR / MINOR)
Evidence: "<exact quote from YAML>"
Explanation: <why this is a problem>

... (repeat for each issue)

### Verdict
PASS | MINOR ISSUES | MAJOR ERRORS
<One-sentence summary.>
```

If no issues are found in a category, skip it. End with the verdict.

---

**Step 4 â€” Update this file.**
After producing the report, use `Filesystem:edit_file` to update the progress tracker table in this file (`/Users/arkinjo/work/GitHub/togo-mcp/evaluation2/togomcp_qa_prompt.md`):

1. Replace the `â€”` in the **Status** column with the verdict code:
   - `PASS` â†’ `P`
   - `MINOR ISSUES` â†’ `W`
   - `MAJOR ERRORS` â†’ `F`
2. Replace the `â€”` in the **Issues** column with a comma-separated list of triggered error codes (e.g., `C11, C18`). If no issues, leave `â€”`.
3. Recount and update the **Summary** line at the bottom of the table accordingly.

Example: if question 006 has minor issues C11 and C18, change:
```
| 006 | â€” | â€” |
```
to:
```
| 006 | W | C11, C18 |
```

---

## Progress Tracker

Mark: `P` = pass Â· `W` = minor issues Â· `F` = major errors

| #   | Status | Issues |
|-----|--------|--------|
| 001 | P      | â€”      |
| 002 | P      | â€”      |
| 003 | P      | â€”      |
| 004 | P      | â€”      |
| 005 | P      | â€”      |
| 006 | P      | â€”      |
| 007 | P      | â€”      |
| 008 | P      | â€”      |
| 009 | P      | â€”      |
| 010 | P      | â€”      |
| 011 | P      | â€”        |
| 012 | P      | â€”      |
| 013 | P      | â€”      |
| 014 | P      | â€”      |
| 015 | P      | â€”      |
| 016 | P      | â€”      |
| 017 | P      | â€”      |
| 018 | P      | â€”      |
| 019 | P      | â€”      |
| 020 | P      | â€”      |
| 021 | P      | â€”      |
| 022 | P      | â€”      |
| 023 | P      | â€”        |
| 024 | P      | â€”      |
| 025 | P      | â€”      |
| 026 | P      | â€”      |
| 027 | P      | â€”      |
| 028 | P      | â€”        |
| 029 | P      | â€”      |
| 030 | P      | â€”        |
| 031 | P      | â€”      |
| 032 | P      | â€”        |
| 033 | P      | â€”        |
| 034 | P      | â€”      |
| 035 | P      | â€”      |
| 036 | P      | â€”      |
| 037 | P      | â€”      |
| 038 | P      | â€”      |
| 039 | P      | â€”      |
| 040 | P      | â€”      |
| 041 | P      | â€”      |
| 042 | P      | â€”      |
| 043 | P      | â€”      |
| 044 | W      | C18    |
| 045 | P      | â€”      |
| 046 | P      | â€”      |
| 047 | P      | â€”      |
| 048 | P      | â€”      |
| 049 | P      | â€”      |
| 050 | P      | â€”      |

**Summary:** `P` = 49 &nbsp;Â·&nbsp; `W` = 1 &nbsp;Â·&nbsp; `F` = 0 &nbsp;Â·&nbsp; Reviewed: 50 / 50

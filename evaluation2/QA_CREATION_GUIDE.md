# TogoMCP Question Creation Guide (v4.3)

Create 50 evaluation questions testing TogoMCP's ability to answer biological questions using RDF databases.

---

## âš ï¸ CRITICAL: EXECUTION-FIRST PHILOSOPHY âš ï¸

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                    â•‘
â•‘  THIS GUIDE REQUIRES ACTUAL TOOL EXECUTION, NOT PLANNING          â•‘
â•‘                                                                    â•‘
â•‘  âŒ WRONG: "I will write a SPARQL query to..."                    â•‘
â•‘  âœ… RIGHT: [calls run_sparql(), pastes results]                   â•‘
â•‘                                                                    â•‘
â•‘  WRITING WITHOUT EXECUTING = INVALID QUESTION                     â•‘
â•‘                                                                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ“‹ OUTPUT FORMAT SPECIFICATION

**â­ CANONICAL FORMAT: See `QUESTION_FORMAT.md` for complete specification â­**

All question files MUST follow the YAML format defined in `QUESTION_FORMAT.md`, which specifies:
- Required and optional fields
- Field types and constraints
- Format by question type
- RDF triples comment format
- Validation rules
- Complete examples

**Quick reference** - Required top-level fields:
```yaml
id, type, body, inspiration_keyword, togomcp_databases_used,
verification_score, pubmed_test, sparql_queries, rdf_triples,
exact_answer, ideal_answer, question_template_used, time_spent
```

---

## QUESTION COMPLETION CHECKLIST

**âš ï¸ CHECK EACH ITEM AS YOU COMPLETE IT âš ï¸**

```yaml
PRE-WORKFLOW:
â–¡ Read coverage_tracker.yaml - DOCUMENTED current percentages below
â–¡ Identified featured database (underutilized, not >45%)
â–¡ Called get_MIE_file() - DOCUMENTED kw_search_tools below
â–¡ Selected keyword from keywords.tsv

RDF NECESSITY GATES (BOTH MUST PASS):
â–¡ Training knowledge test completed - PASS (cannot answer from memory)
â–¡ Search/API tools test completed - PASS (cannot answer with search tools alone)

DISCOVERY (ALL TOOLS MUST BE EXECUTED):
â–¡ Called search API from kw_search_tools - RESULTS PASTED below
â–¡ Executed SPARQL structure query - RESULTS PASTED below  
â–¡ Executed final SPARQL query - RESULTS PASTED below
â–¡ All queries returned NON-EMPTY results (or investigated why)

VALIDATION (ALL TOOLS MUST BE EXECUTED):
â–¡ Called PubMed:search_articles - AT LEAST 2 queries, PMIDs LISTED below

FINAL:
â–¡ Score â‰¥9/12 calculated and justified
â–¡ All fields in question_XXX.yaml filled (no placeholders)
â–¡ coverage_tracker.yaml updated with new counts
â–¡ question_XXX.yaml follows QUESTION_FORMAT.md specification

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
IF ANY BOX UNCHECKED: QUESTION IS INCOMPLETE AND INVALID
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## CORE PRINCIPLES

1. **Biology First**: Ask questions researchers care about (not database inventory)
2. **Database Balance First**: Check coverage before every question
3. **RDF Necessity**: Must require current database state (not PubMed or training knowledge)
4. **TWO Critical Gates**: Both Training Test AND Search/API Test must PASS
5. **Integration-Driven**: 60%+ integrate 2+ databases
6. **Verifiable Scope**: Bounded, objectively checkable answers
7. **Comprehensive Analysis**: For yes/no questions, use comprehensive SPARQL (not example-based validation)
8. **EXECUTION REQUIRED**: Every tool mentioned must be actually called with results documented
9. **CANONICAL FORMAT**: Follow QUESTION_FORMAT.md specification exactly

---

## FILE LOCATIONS

```
FORMAT:  /Users/arkinjo/work/GitHub/togo-mcp/evaluation2/QUESTION_FORMAT.md â­ CANONICAL SPECIFICATION
Input:   /Users/arkinjo/work/GitHub/togo-mcp/evaluation2/keywords.tsv
Track:   /Users/arkinjo/work/GitHub/togo-mcp/evaluation2/questions/coverage_tracker.yaml
Output:  /Users/arkinjo/work/GitHub/togo-mcp/evaluation2/questions/question_XXX.yaml
```

**Tools:**
- **User files:** `Filesystem:read_text_file()` / `Filesystem:write_file()`
- **RDF databases:** `TogoMCP-Test:get_MIE_file()` / `run_sparql()` / `search_*_entity()` / `ncbi_esearch()`
- **Ontology APIs:** `OLS4:searchClasses()` / `getDescendants()` / `getAncestors()`

---

## REQUIREMENTS

### Database Coverage (All 23 Required)
- **Tier 1 (â‰¥3 each):** UniProt, ChEBI, ChEMBL, Rhea, PubChem, MeSH, GO, Reactome, Taxonomy, Ensembl, NCBI_Gene
- **Tier 2-4 (â‰¥1 each):** PDB, ClinVar, MedGen, PubMed, AMRPortal, BacDive, MediaDive, DDBJ, NANDO, PubTator, Glycosmos, MONDO

### Balance Quotas (CRITICAL)
- **UniProt: â‰¤35 questions (70% max)** - Target 30-40%
- **GO: â‰¤25 questions (50% max)** - Target 24-30%
- **Skip databases >45%** for next 3+ questions
- **Prioritize databases <5%**
- **60%+ questions must NOT use UniProt**

### Question Types & Integration
- Factoid: 10 | Yes/No: 10 | List: 10 | Summary: 10 | Choice: 20
- **60%+ integrate 2+ databases** via cross-references or semantic links

### Quality Standards
- All questions score â‰¥9/12 (see Scoring Rubric)
- **Pass BOTH gates: Training Knowledge Test AND Search/API Tools Test**
- PubMed test shows non-answerability (must CALL the tool)
- Precise wording (avoid ambiguity)
- **Follow QUESTION_FORMAT.md specification**

---

## WORKFLOW (10 STEPS WITH MANDATORY EXECUTION)

### Step 1: Check Balance (MANDATORY FIRST STEP)

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ›‘ EXECUTION CHECKPOINT 1: BALANCE CHECK ğŸ›‘
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
YOU MUST EXECUTE: Filesystem:read_text_file(coverage_tracker.yaml)

PASTE RESULTS HERE:
[Current database percentages]

DECISION DOCUMENTED:
Featured database: [name]
Current coverage: [X%]
Reason: [underutilized/never used/strategic choice]

Status: â–¡ COMPLETE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Critical**: Never start without checking balance. This prevents wasting effort on overused databases.

**Balance Rules:**
- Skip databases >45% for next 3+ questions
- Prioritize databases <5%
- Never exceed UniProt 70%, GO 50%

---

### Step 2: Read MIE File & Select Keyword

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ›‘ EXECUTION CHECKPOINT 2: MIE FILE READ ğŸ›‘
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
YOU MUST EXECUTE: TogoMCP-Test:get_MIE_file(dbname="featured_database")

DOCUMENT CRITICAL FIELDS:
kw_search_tools: [list the tools from MIE]
Example SPARQL patterns: [note patterns]
Key predicates: [list structured properties]

Status: â–¡ COMPLETE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**â­ ALWAYS CHECK MIE's `kw_search_tools` FIELD FIRST â­**

**Why This Matters:**
- MIE explicitly tells you which API works for keyword discovery
- Wrong: Jump to SPARQL for keywords â†’ Empty results
- Right: Use kw_search_tools â†’ Get IDs â†’ SPARQL for structure

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ›‘ EXECUTION CHECKPOINT 2B: KEYWORD SELECTION ğŸ›‘
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
YOU MUST EXECUTE: Filesystem:read_text_file(keywords.tsv)

DOCUMENT SELECTION:
Selected keyword: [KW-XXXX]
Keyword name: [name]
Category: [category]
Match to database: [why this keyword fits featured database]

Question formulated: [exact question text]

Wording check: 
â–¡ No ambiguous verbs (bind/contain/have/associated with)
â–¡ Scope is clear
â–¡ Featured database is PRIMARY
â–¡ No database names in question text

Status: â–¡ COMPLETE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Question Formulation - Precise Wording:**

Avoid ambiguity by being specific about:
1. **Native vs. experimental**: "native cofactor" not just "bind"
2. **Annotation vs. structure**: "annotated with" vs "crystallized with"
3. **Total vs. filtered counts**: Make scope explicit

**Red flag verbs requiring qualification:**
- bind, contain, have, associated with, interact with, found in, related to
- **Solution**: Add qualifiers like "natively", "annotated as", "experimentally determined"

**Examples:**
```yaml
âŒ Vague: "Which proteins bind magnesium?"
âœ… Precise: "Which proteins are annotated with native magnesium cofactor binding?"

âŒ Vague: "Which genes are associated with hypertension?"
âœ… Precise: "Which genes have pathogenic variants annotated for hypertension in ClinVar?"
```

---

### Step 3: RDF Necessity Tests (TWO MANDATORY GATES - BOTH MUST PASS)

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ›‘ CRITICAL: TWO GATES - BOTH MUST PASS ğŸ›‘
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Gate 3A: Training Knowledge Test
Gate 3B: Search/API Tools Test

âš ï¸ IF EITHER GATE FAILS â†’ STOP and redesign question âš ï¸
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

#### Step 3A: Training Knowledge Test (GATE 1)

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ›‘ EXECUTION CHECKPOINT 3A: TRAINING KNOWLEDGE TEST (GATE 1) ğŸ›‘
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âš ï¸ THIS IS GATE 1: IF YOU CAN ANSWER FROM MEMORY â†’ REJECT QUESTION âš ï¸

Question: Can I answer this question from my training knowledge alone?

Attempted answer from memory: [your answer based on training knowledge]
Confidence level: [high | medium | low | none]
Reasoning: [explain why you can or cannot answer]

DECISION:
â–¡ PASS (cannot answer from memory â†’ proceed to Gate 2)
â–¡ FAIL (can answer from memory â†’ STOP and redesign)

âš ï¸ If FAIL: STOP HERE. Redesign question or select different topic.

Status: â–¡ COMPLETE (result is PASS)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Understanding Gate 1:**

**PASS (Good questions - cannot answer from memory):**
```yaml
âœ… "How many human proteins have BOTH PDB structures AND ClinVar disease variants?"
   Cannot answer: Requires cross-referencing two databases with current data

âœ… "How many GO biological process terms have Reactome cross-references?"
   Cannot answer: Need to count across entire GO database

âœ… "Which bacterial strain in BacDive has the highest optimal growth temperature?"
   Cannot answer: Requires querying cultivation data across all strains

âœ… "How many human protein kinases are targeted by FDA-approved drugs?"
   Cannot answer: Need exact count from ChEMBL's drug-target-mechanism data
```

**FAIL (Bad questions - can answer from memory):**
```yaml
âŒ "How many reviewed proteins in UniProt are annotated with nitrogen fixation?"
   Can answer: ~700-800 proteins (nitrogenase complex, nif genes)

âŒ "What organisms perform nitrogen fixation?"
   Can answer: Rhizobium, Azotobacter, cyanobacteria

âŒ "What is the function of hemoglobin?"
   Can answer: Oxygen transport via heme groups

âŒ "What is the structure of ATP?"
   Can answer: Adenosine triphosphate, three phosphate groups
```

---

#### Step 3B: Search/API Tools Test (GATE 2) âš ï¸ NEW

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ›‘ EXECUTION CHECKPOINT 3B: SEARCH/API TOOLS TEST (GATE 2) ğŸ›‘
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âš ï¸ THIS IS GATE 2: IF SEARCH TOOLS CAN ANSWER â†’ REJECT QUESTION âš ï¸

Question: Can search/API tools answer this WITHOUT using SPARQL/RDF?

Tools to test: [list relevant API tools from kw_search_tools]

YOU MUST EXECUTE: Test with actual API calls

Test execution:
Tool: [exact tool name]
Query: [exact parameters]
Result: [paste actual results]

PASTE ACTUAL TOOL OUTPUT HERE:
[tool results]

Analysis: [Can this answer the question? Why or why not?]

DECISION:
â–¡ PASS (search tools CANNOT fully answer â†’ proceed to Step 4)
â–¡ FAIL (search tools CAN answer â†’ STOP and redesign)

âš ï¸ If FAIL: STOP HERE. Redesign question to require RDF capabilities.

Status: â–¡ COMPLETE (result is PASS, tool output pasted)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Understanding Gate 2:**

**PASS (Search tools insufficient - requires RDF):**
```yaml
âœ… "How many GO biological process terms have Reactome cross-references?"
   - OLS4:searchClasses finds examples but cannot:
     * Filter by hasOBONamespace property (biological_process)
     * Check hasDbXref property existence
     * Aggregate counts across entire database
   - Requires SPARQL for property filtering + aggregation

âœ… "How many GO terms have EXACTLY 3 direct children?"
   - OLS4:getDescendants shows descendants but cannot:
     * Aggregate counts across all terms
     * Filter for exactly 3 children
   - Requires SPARQL to iterate and count

âœ… "Which proteins have kinase activity AND >5 disease variants?"
   - Search finds examples but cannot:
     * Join data from multiple sources
     * Apply complex filtering across databases
   - Requires SPARQL for cross-database integration

âœ… "How many human protein kinases are targeted by FDA-approved drugs?"
   - search_chembl_target finds kinases but cannot:
     * Filter by drug development phase (phase 4)
     * Link to drug mechanisms
     * Aggregate across molecule-mechanism-target relationships
   - Requires SPARQL for multi-entity joins
```

**FAIL (Search tools CAN answer - question invalid):**
```yaml
âŒ "How many direct children does GO:0097190 have?"
   - OLS4:getDescendants returns all descendants with directParent info
   - Can filter by directParent and count
   - Does NOT require SPARQL

âŒ "What is the molecular formula of aspirin?"
   - search_chembl_molecule or search_pubchem returns this directly
   - Simple lookup, not RDF-specific

âŒ "List the synonyms of GO:0006915"
   - OLS4:search returns all synonyms in response
   - No RDF querying needed
```

**Key Distinction:**
- **Can answer with search tools** = API provides complete answer â†’ **REJECT**
- **Cannot answer with search tools** = Needs RDF property filtering, aggregation, or joins â†’ **ACCEPT**

---

### Step 4: Search API Discovery

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ›‘ EXECUTION CHECKPOINT 4: SEARCH API DISCOVERY ğŸ›‘
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
YOU MUST EXECUTE: [tool from MIE's kw_search_tools]

Tool called: [exact tool name]
Query used: [exact query string]
Results count: [total number found]
Example IDs: [list at least 5]

Purpose: Find examples for SPARQL query design (NOT for answering question)

PASTE ACTUAL TOOL OUTPUT HERE:
[tool results]

Status: â–¡ COMPLETE (results pasted above)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Critical**: Get 5-10 example IDs to understand data patterns before writing SPARQL.

**Important Notes:**
- These IDs are for query design ONLY
- Do NOT use these IDs in VALUES for comprehensive queries
- This step passed Gate 2 because these examples alone cannot answer the question

---

### Step 5: SPARQL Structure Examination

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ›‘ EXECUTION CHECKPOINT 5: STRUCTURE EXAMINATION ğŸ›‘
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
YOU MUST EXECUTE: run_sparql() to examine entity properties

Query executed: [paste query]

PASTE RESULTS (first 10-20 rows):
[actual SPARQL results]

Key properties discovered: [list important predicates]

IF EMPTY RESULTS: 
â–¡ I investigated why (describe investigation)
â–¡ I fixed the query (show corrected version)

Status: â–¡ COMPLETE (non-empty results pasted OR investigation documented)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Structure Query Pattern:**
```sparql
# Example: Examine properties of example entities
SELECT ?entity ?property ?value
WHERE {
  VALUES ?entity {
    <example_id_1>
    <example_id_2>
    <example_id_3>
  }
  ?entity ?property ?value .
}
LIMIT 50
```

---

### Step 6: Strategy Decision & Final SPARQL

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ›‘ EXECUTION CHECKPOINT 6A: STRATEGY DECISION ğŸ›‘
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
DOCUMENTED DECISION:

Question type: [factoid/yesno/list/summary/choice]
Strategy: [comprehensive | example-based]
Justification: [why this strategy]

For yes/no: â–¡ Using comprehensive SPARQL (bif:contains + synonyms)
            â–¡ NOT using VALUES with search results

For factoid/count: â–¡ Using comprehensive aggregation
                   â–¡ NOT limiting to search result IDs

Status: â–¡ COMPLETE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Strategy Guidelines:**
- **Comprehensive**: Use for counts, yes/no, "all/which/how many" questions
- **Example-based**: Only for "name one example" or bounded lists with explicit limits

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ›‘ EXECUTION CHECKPOINT 6B: FINAL SPARQL EXECUTION ğŸ›‘
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
YOU MUST EXECUTE: run_sparql() with final query

Database: [dbname or endpoint_name]
Query executed: [paste complete query]

PASTE COMPLETE RESULTS:
[all rows returned, or first 50 if many]

Answer verified: [Yes/No - can I answer the question from these results?]
Answer extracted: [the actual answer]

IF INTEGRATION: 
â–¡ Tested cross-database links work
â–¡ Results from both databases present

Status: â–¡ COMPLETE (results pasted AND answer verified)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

### Step 7: Integration Testing (if multi-database)

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ›‘ EXECUTION CHECKPOINT 7: INTEGRATION (IF MULTI-DB) ğŸ›‘
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
If single database:
â–¡ N/A - Single database query

If multi-database:
â–¡ Tested integration between: DB1(_____) Ã— DB2(_____)
â–¡ Integration method: [cross-references | shared endpoint | VALUES pre-filter]
â–¡ Both databases contributed results: [Yes/No]

Integration pattern used: [describe linking strategy]

Status: â–¡ COMPLETE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Integration Patterns:**
- **Cross-references**: Use skos:exactMatch, rdfs:seeAlso, etc.
- **Shared endpoint**: Query multiple GRAPHs in single SPARQL
- **VALUES pre-filter**: Get IDs from DB1, use in DB2 query

---

### Step 8: PubMed Test (15 minutes maximum)

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ›‘ EXECUTION CHECKPOINT 8: PUBMED TEST ğŸ›‘
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
YOU MUST EXECUTE: PubMed:search_articles (minimum 2 queries)

Query 1:
  Exact query string: [paste here]
  Tool called: [Yes/No] â† MUST BE YES
  PMIDs returned: [list all]
  Total found: [count]
  Why insufficient: [explain why these papers don't answer the question]

Query 2:
  Exact query string: [paste here]
  Tool called: [Yes/No] â† MUST BE YES
  PMIDs returned: [list all]
  Total found: [count]
  Why insufficient: [explain why these papers don't answer the question]

Conclusion: [why RDF databases are essential for this question]

Status: â–¡ COMPLETE (at least 2 queries executed, PMIDs listed)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Why Papers Are Insufficient (Common Reasons):**
- Don't provide exact counts/comprehensive data
- Outdated compared to current database state
- Discuss topic generally but lack specific answer
- Would require manual compilation from multiple sources
- No access to structured, queryable relationships

---

### Step 9: Score & Validate

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ›‘ EXECUTION CHECKPOINT 9: SCORING & VALIDATION ğŸ›‘
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
CALCULATE SCORE:

Biological Insight: [0/1/2/3] - Justification: [explain]
Multi-Database: [0/1/2/3] - Justification: [explain]
Verifiability: [0/1/2/3] - Justification: [explain]
RDF Necessity: [0/1/2/3] - Justification: [explain]

TOTAL: [sum] / 12

Minimum required: 9/12
Result: â–¡ PASS â–¡ FAIL

Status: â–¡ COMPLETE (score â‰¥9/12)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Scoring Rubric (0-3 per dimension, total â‰¥9/12):**

| Dimension | 3 | 2 | 1 | 0 |
|-----------|---|---|---|---|
| **Biological Insight** | Mechanisms/patterns | Functional relationships | Simple facts | Database inventory |
| **Multi-Database** | 3+ DBs integrated | 2 DBs integrated | Single DB + references | Search-only |
| **Verifiability** | Single/â‰¤5 items | â‰¤10 items | â‰¤20 items | Unbounded |
| **RDF Necessity** | Impossible without RDF | Very difficult | Possible but tedious | PubMed/training OK |

**Examples by Dimension:**

**Biological Insight:**
- 3: "How do kinase inhibitors achieve selectivity?" (mechanisms)
- 2: "Which kinases interact with CDK4?" (functional relationships)
- 1: "How many kinases are in humans?" (simple fact)
- 0: "List all proteins in UniProt" (database inventory)

**Multi-Database:**
- 3: UniProt + PDB + ClinVar integration
- 2: ChEMBL + ChEBI integration
- 1: UniProt with GO cross-references
- 0: Text search only

**Verifiability:**
- 3: Single count or â‰¤5 ranked items
- 2: 6-10 items
- 1: 11-20 items
- 0: Unbounded or subjective

**RDF Necessity:**
- 3: Requires cross-database joins, property filtering, aggregation
- 2: Complex SPARQL needed but possible without RDF
- 1: Could compile manually from papers with effort
- 0: Available in training data or PubMed

---

### Step 10: Document & Update

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ›‘ EXECUTION CHECKPOINT 10: DOCUMENTATION ğŸ›‘
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â­ FOLLOW CANONICAL FORMAT: See QUESTION_FORMAT.md for complete specification

YOU MUST EXECUTE: 
1. Filesystem:write_file(question_XXX.yaml) - following QUESTION_FORMAT.md
2. Filesystem:write_file(coverage_tracker.yaml - UPDATED)

Question file created: â–¡ Yes
  - Follows QUESTION_FORMAT.md structure: â–¡ Yes
  - All required fields present: â–¡ Yes
    âœ“ id, type, body
    âœ“ inspiration_keyword (with keyword_id, name, category)
    âœ“ togomcp_databases_used
    âœ“ verification_score (with biological_insight, multi_database, verifiability, rdf_necessity, total, passed)
    âœ“ pubmed_test (with time_spent, method, result, conclusion)
    âœ“ sparql_queries (array with query_number, database, description, query, result_count)
    âœ“ rdf_triples (Turtle format with comments: # Database: X | Query: N | Comment: ...)
    âœ“ exact_answer (format matches question type)
    âœ“ ideal_answer (one paragraph for experts)
    âœ“ question_template_used
    âœ“ time_spent (exploration, formulation, verification, pubmed_test, extraction, documentation, total)
  - All fields filled (no placeholders): â–¡ Yes
  - SPARQL queries show ACTUAL execution: â–¡ Yes
  - PubMed test shows ACTUAL PMIDs: â–¡ Yes
  - RDF triples follow comment format: â–¡ Yes

Coverage tracker updated: â–¡ Yes
  - Counts incremented: â–¡ Yes
  - Percentages recalculated: â–¡ Yes
  - Question ID added to database list: â–¡ Yes

Status: â–¡ COMPLETE

âš ï¸ VALIDATE: Check your YAML against QUESTION_FORMAT.md specification
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Output Format Checklist:**

```yaml
Required Structure (from QUESTION_FORMAT.md):

â–¡ id: question_XXX (matches filename)
â–¡ type: [yes_no | factoid | list | summary]
â–¡ body: "Question without database names"
â–¡ inspiration_keyword:
    keyword_id: KW-XXXX
    name: "Name"
    category: "Category"
â–¡ togomcp_databases_used: [array of databases]
â–¡ verification_score:
    biological_insight: [0-3]
    multi_database: [0-3]
    verifiability: [0-3]
    rdf_necessity: [0-3]
    total: [0-12]
    passed: true
â–¡ pubmed_test:
    time_spent: "15 minutes"
    method: "Description"
    result: "What was found"
    conclusion: "PASS (...)"
â–¡ sparql_queries: [array with all required sub-fields]
â–¡ rdf_triples: "Turtle format with mandatory comments"
â–¡ exact_answer: [format matches type]
â–¡ ideal_answer: "One paragraph"
â–¡ question_template_used: "Template N"
â–¡ time_spent: [all phases documented]
```

**RDF Triples Comment Format:**
```turtle
<subject> <predicate> <object> .
# Database: [database_name] | Query: [query_number] | Comment: [relevance]
```

---

## COMPLETE WORKFLOW SUMMARY

```
1. CHECK BALANCE FIRST (coverage_tracker.yaml)
   â””â”€ EXECUTE: read file, document percentages
   â†“
2. READ MIE FILE & SELECT KEYWORD (check kw_search_tools!)
   â””â”€ EXECUTE: get_MIE_file(), read keywords.tsv, document selection
   â†“
3A. TRAINING KNOWLEDGE TEST (GATE 1: must PASS = cannot answer)
   â””â”€ DOCUMENT: answer from memory, confidence, reasoning
   â””â”€ If FAIL (can answer) â†’ STOP and redesign
   â†“
3B. SEARCH/API TOOLS TEST (GATE 2: must PASS = cannot answer with APIs)
   â””â”€ EXECUTE: test with API tools â†’ paste results
   â””â”€ If FAIL (APIs can answer) â†’ STOP and redesign
   â†“
4. SEARCH API DISCOVERY
   â””â”€ EXECUTE: search API â†’ paste results (for query design, not answer)
   â†“
5. SPARQL STRUCTURE
   â””â”€ EXECUTE: SPARQL structure â†’ paste results
   â†“
6. STRATEGY DECISION & FINAL SPARQL
   â””â”€ DOCUMENT: strategy choice (comprehensive vs example-based)
   â””â”€ EXECUTE: final SPARQL â†’ paste results, verify answer
   â†“
7. INTEGRATION (if multi-DB)
   â””â”€ TEST: cross-database links work
   â†“
8. PUBMED TEST (15 min, must show insufficiency)
   â””â”€ EXECUTE: PubMed:search_articles x2 â†’ list PMIDs
   â†“
9. SCORE & VALIDATE (â‰¥9/12, checklist)
   â””â”€ CALCULATE: score each dimension, justify
   â†“
10. DOCUMENT & UPDATE (â­ FOLLOW QUESTION_FORMAT.md)
   â””â”€ EXECUTE: write question file following canonical format
   â””â”€ EXECUTE: update coverage tracker (new counts + percentages)
   â””â”€ VALIDATE: check all required fields against QUESTION_FORMAT.md
```

---

## SUCCESS CRITERIA

**Every question must:**
1. âœ… Pass Training Knowledge Test (CANNOT answer from memory)
2. âœ… Pass Search/API Tools Test (CANNOT answer with search tools alone)
3. âœ… Score â‰¥9/12 (no dimension = 0)
4. âœ… Use precise wording (no ambiguous verbs)
5. âœ… Feature database as PRIMARY (not peripheral)
6. âœ… Use comprehensive SPARQL (for yes/no questions)
7. âœ… Fail PubMed test (requires RDF)
8. âœ… Maintain database balance (check tracker first)
9. âœ… Use structured properties (no text filters when possible)
10. âœ… Have bounded, verifiable scope
11. âœ… **SHOW PROOF OF EXECUTION for all tools**
12. âœ… **ALL SPARQL queries executed with non-empty results**
13. âœ… **PubMed:search_articles called with PMIDs listed**
14. âœ… **Search/API tools tested with actual results pasted**
15. âœ… **Follow QUESTION_FORMAT.md specification exactly**

**By end of 50 questions:**
- All 23 databases covered (Tier 1 â‰¥3, Tier 2-4 â‰¥1)
- UniProt â‰¤70%, GO â‰¤50%
- 60%+ integrate 2+ databases
- 10 each of factoid, yes/no, list, summary
- 20 choice questions

---

## FINAL SELF-CHECK BEFORE SUBMITTING

```
â“ ANSWER HONESTLY FOR EVERY QUESTION:

TWO GATES:
â–¡ I attempted to answer from memory first (Gate 3A)
â–¡ I CANNOT answer this question from training knowledge (PASS)
â–¡ I tested with search/API tools (Gate 3B)
â–¡ Search tools CANNOT fully answer this question (PASS)
â–¡ If either gate failed, I redesigned the question

EXECUTION PROOF:
â–¡ I called the search API and pasted actual results
â–¡ I tested search/API tools and pasted results (Gate 3B)
â–¡ I called run_sparql() at least twice and pasted results
â–¡ I called PubMed:search_articles at least twice
â–¡ All my SPARQL queries returned non-empty results

DOCUMENTATION:
â–¡ My question_XXX.yaml shows actual tool outputs (not plans)
â–¡ PMIDs are listed (not "various papers")
â–¡ Example IDs are listed (not "found some entities")
â–¡ Search/API test results are documented

FORMAT COMPLIANCE:
â–¡ My question_XXX.yaml follows QUESTION_FORMAT.md specification
â–¡ All required fields are present
â–¡ RDF triples use correct comment format: # Database: X | Query: N | Comment: ...
â–¡ exact_answer format matches question type
â–¡ verification_score.passed is true
â–¡ time_spent includes all phases

VERIFICATION:
â–¡ I can point to tool output proving my answer
â–¡ Score is calculated with justification
â–¡ Score â‰¥9/12 with no dimension = 0

IF ANY â–¡ UNCHECKED: QUESTION IS INVALID
```

---

## VERSION HISTORY

- **v4.3** (2025-02-11): Restored full guide with all detailed workflow steps; added explicit references to QUESTION_FORMAT.md canonical specification throughout; aligned step numbering with WORKSHEET (10 steps with 3A/3B gates)
- **v4.2** (2025-02-11): Added Step 3B (Search/API Tools Test) as mandatory second gate to filter out questions answerable by API tools alone
- **v4.1** (2025-02-11): Corrected Training Knowledge Test logic (PASS = cannot answer from memory)
- **v4.0** (2025-02-11): Added mandatory execution checkpoints and blocking gates
- **v3.0** (2025-02-11): Integrated BioASQ advantages with comprehensive scoring
- **v2.0** (2025-02-10): Initial QA_CREATION_GUIDE with MIE kw_search_tools emphasis

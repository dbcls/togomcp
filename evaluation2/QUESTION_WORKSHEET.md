# Question Creation Worksheet

**‚ö†Ô∏è FILL EVERY BLANK OR QUESTION IS INVALID ‚ö†Ô∏è**

**üìã OUTPUT FORMAT: See QUESTION_FORMAT.md for canonical YAML specification**

---

## Q___ : _________________ (question type)

### 1. BALANCE CHECK
```
‚ñ° Read coverage_tracker.yaml
Featured database: _______________ (current %: _____)
Reason: _______________

‚ö†Ô∏è DATASET COMPOSITION TARGET:
Multi-database questions: Target 60-80% of total (30-40 out of 50)
Single-database questions: Maximum 20-40% (10-20 out of 50)

Current status: _____ multi-DB / _____ total = _____%
Decision: ‚ñ° Multi-database (2-4 DBs) ‚ñ° Single-database (justify: _______________)
```

### 2. MIE & KEYWORD
```
‚ñ° get_MIE_file(dbname="_____________")
kw_search_tools: _______________
Keyword selected: KW-____ (_______________)
```

### 3. RDF NECESSITY TESTS ‚ö†Ô∏è CRITICAL FILTERS - BOTH MUST PASS

#### 3A. TRAINING KNOWLEDGE TEST
```
Question: Can I answer this from training knowledge alone?
My answer attempt: _______________
Confidence: _______________ (none/low/medium/high)

Result: ‚ñ° PASS (cannot answer ‚Üí proceed to 3B)
        ‚ñ° FAIL (can answer ‚Üí reject and redesign)

PASS examples (cannot answer from memory):
  - "How many human proteins have BOTH PDB structures AND disease variants?"
  - "Which Rhea reactions are catalyzed by UniProt protein P12345?"
  - "What is the exact count of proteins with phosphorylation at position 100-110?"
  
FAIL examples (can answer from memory):
  - "How many reviewed nitrogen fixation proteins are in UniProt?" 
    ‚Üí Can estimate ~700-800 from training
  - "What organisms perform nitrogen fixation?" 
    ‚Üí Know: Rhizobium, Azotobacter, cyanobacteria
  - "What is the function of nitrogenase?" 
    ‚Üí Know: converts N2 to NH3
```

#### 3B. SEARCH/API TOOLS TEST ‚ö†Ô∏è CRITICAL - MUST BE HONEST

**CORE PRINCIPLE: IF YOU CAN ANSWER IT WITH THE TOOLS, THEN THE TOOLS CAN ANSWER IT.**

```
Question: Can search/API tools answer this question WITHOUT using RDF/SPARQL?

‚ö†Ô∏è ANTI-RATIONALIZATION RULES:
1. "Requires manual parsing" = TOOLS CAN ANSWER (parsing is trivial)
2. "Requires counting results" = TOOLS CAN ANSWER (counting is trivial)
3. "Requires aggregating data" = TOOLS CAN ANSWER (if data is all in responses)
4. "No built-in GROUP BY" = NOT A VALID EXCUSE (users can group/count themselves)
5. If YOU successfully answered it with tools ‚Üí TEST FAILS, question is INVALID

MANDATORY EXECUTION (ACTUALLY TRY TO ANSWER THE QUESTION):
‚ñ° Step 1: Call tool 1: _______________(params="_____________")
  Result: _______________
  
‚ñ° Step 2: Call tool 2 (if needed): _______________(params="_____________")
  Result: _______________
  
‚ñ° Step 3: Processing needed: _______________
  (e.g., "extract genus from scientific name", "count unique values", "parse JSON field")
  
‚ñ° Step 4: Did I successfully get the answer? ‚ñ° Yes ‚ñ° No
  
  If YES ‚Üí Answer obtained: _______________
          TEST RESULT: FAIL (tools CAN answer)
          ACTION: REJECT this question
          
  If NO ‚Üí Why tools failed: _______________
         TEST RESULT: PASS (tools cannot answer)
         ACTION: Proceed to design SPARQL queries

‚ö†Ô∏è CRITICAL EVALUATION CRITERIA:

TOOLS CAN ANSWER (TEST FAILS) when:
- All required data is in API responses
- Processing = simple parsing, filtering, counting, grouping
- Example: Extract genus from organism names + count unique ‚Üí TRIVIAL
- Example: Parse JSON field + filter by value ‚Üí TRIVIAL
- Example: Retrieve N results + apply local aggregation ‚Üí TRIVIAL
- If you got the answer by calling tools, TEST FAILS regardless of "complexity"

TOOLS CANNOT ANSWER (TEST PASSES) when:
- Data requires graph traversal not exposed by API
- Multiple databases need joining beyond API capabilities  
- Requires ontology reasoning not available in search
- Computational complexity exceeds practical limits (e.g., cross-product of millions)
- Example: "Count GO terms with EXACTLY 3 children" ‚Üí must check ALL terms, API doesn't expose this
- Example: "Proteins with kinase activity AND disease variants" ‚Üí cross-DB join not in single API

PASS examples (tools genuinely insufficient):
  ‚úì "How many GO terms have EXACTLY 3 direct children?"
    ‚Üí getDescendants shows one term's descendants
    ‚Üí Would need to call for EVERY GO term (100,000+) then aggregate
    ‚Üí Computationally impractical, RDF query needed
    
  ‚úì "Which proteins have kinase activity (GO) AND disease variants (ClinVar)?"
    ‚Üí No single API joins UniProt + GO + ClinVar
    ‚Üí Would need separate searches + manual cross-referencing
    ‚Üí RDF enables direct cross-database join
    
  ‚úì "What percentage of Rhea reactions involve ATP?"
    ‚Üí search_rhea_entity("ATP") returns SOME reactions
    ‚Üí But denominator requires counting ALL reactions
    ‚Üí Would need text search on all 18,000+ reactions
    ‚Üí RDF enables precise ChEBI IRI filtering

FAIL examples (tools CAN answer - REJECT these):
  ‚úó "How many distinct genera have nifH genes?"
    ‚Üí ncbi_esearch gets all genes
    ‚Üí ncbi_esummary gets organism names
    ‚Üí Extract genus (first word) + count unique
    ‚Üí Answer obtained with tools ‚Üí REJECT
    
  ‚úó "What is the molecular formula of aspirin?"
    ‚Üí search_chembl_molecule("aspirin") returns formula directly
    ‚Üí Answer obtained with tools ‚Üí REJECT
    
  ‚úó "List human genes on chromosome 7"
    ‚Üí ncbi_esearch(query="Homo sapiens[Organism] AND 7[Chromosome]")
    ‚Üí Returns gene list directly
    ‚Üí Answer obtained with tools ‚Üí REJECT

HONESTY CHECK:
‚ñ° Did I actually TRY to answer with tools? (not just theorize)
‚ñ° If I got an answer, did I mark TEST as FAIL?
‚ñ° Am I being honest about what "trivial processing" means?
‚ñ° Would a competent user be able to answer this with the tools I tested?

Result: ‚ñ° PASS (tools cannot answer ‚Üí requires RDF)
        ‚ñ° FAIL (tools CAN answer ‚Üí REJECT question and redesign)
```

### 4. SEARCH API (MUST EXECUTE)
```
‚ñ° Tool: _______________(query="_____________")
Total results: _____
Example IDs: _____ _____ _____ _____ _____
Purpose: Find examples for SPARQL query design (not for answering question)
```

### 5. SPARQL STRUCTURE (MUST EXECUTE)
```
‚ñ° run_sparql(dbname="_____", query=...)
Key properties found: _______________ _______________ _______________
Results: ‚ñ° Non-empty ‚ñ° Empty (investigated why: _______)
```

### 6. FINAL SPARQL (MUST EXECUTE)
```
‚ñ° run_sparql(dbname="_____", query=...)
Strategy: ‚ñ° Comprehensive ‚ñ° Example-based
Answer from results: _______________
Verified: ‚ñ° Yes
```

### 7. INTEGRATION (if multi-DB)
```
‚ñ° N/A - Single database
‚ñ° Tested: DB1(_____) √ó DB2(_____)
Integration pattern: _______________
```

### 8. PUBMED TEST (MUST EXECUTE 2x)
```
‚ñ° PubMed:search_articles(query="_____________")
  PMIDs: _______________
  Insufficient because: _______________

‚ñ° PubMed:search_articles(query="_____________")
  PMIDs: _______________
  Insufficient because: _______________
```

### 9. SCORE
```
Biological Insight:  ___/3 (why: _______________)
Multi-Database:      ___/3 (why: _______________)
Verifiability:       ___/3 (why: _______________)
RDF Necessity:       ___/3 (why: _______________)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
TOTAL:              ___/12  (minimum: 9)
```

### 10. FILES
```
‚ñ° question_XXX.yaml written (following QUESTION_FORMAT.md specification)
‚ñ° coverage_tracker.yaml updated (new %: ___%)
```

---

## üìã OUTPUT FORMAT REQUIREMENTS

**Your question_XXX.yaml MUST follow the canonical format in QUESTION_FORMAT.md**

**Required top-level fields:**
```yaml
id: question_XXX
type: [yes_no | factoid | list | summary]
body: "Question text without database names"
inspiration_keyword:
  keyword_id: KW-XXXX
  name: "Keyword name"
  category: "Category"
togomcp_databases_used:
  - database1
  - database2
verification_score:
  biological_insight: [0-3]
  multi_database: [0-3]
  verifiability: [0-3]
  rdf_necessity: [0-3]
  total: [0-12]
  passed: true
pubmed_test:
  time_spent: "15 minutes"
  method: "Description"
  result: "What was found"
  conclusion: "PASS (cannot answer)"
sparql_queries:
  - query_number: 1
    database: "dbname"
    description: "What this query does"
    query: |
      PREFIX declarations
      SELECT ...
    result_count: N
rdf_triples: |
  @prefix declarations
  <subject> <predicate> <object> .
  # Database: X | Query: N | Comment: ...
exact_answer: [varies by type]
ideal_answer: |
  One paragraph synthesis for domain experts
question_template_used: "Template N (Name)"
time_spent:
  exploration: "N minutes"
  formulation: "N minutes"
  verification: "N minutes"
  pubmed_test: "15 minutes"
  extraction: "N minutes"
  documentation: "N minutes"
  total: "N minutes"
```

**See QUESTION_FORMAT.md for:**
- Complete field specifications
- Format requirements by question type
- RDF triples comment format
- Validation rules
- Complete examples

---

**‚úì ALL BOXES CHECKED? ‚Üí Question valid**  
**‚úó ANY BLANK/UNCHECKED? ‚Üí Question invalid**  
**‚úó FORMAT DOESN'T MATCH QUESTION_FORMAT.md? ‚Üí Question invalid**
